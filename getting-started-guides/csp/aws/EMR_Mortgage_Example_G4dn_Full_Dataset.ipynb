{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//this configuration is used for core nodes with 2x g4dn.12xlarge for total 8x T4 GPU\n",
    "//You can also launch the cluster with 2x p3.8xlarge as core nodes, use 8 workers,  6 cores per executor, 36G executor memory & 24G Overhead memory\n",
    "\n",
    "\n",
    "%%configure -f\n",
    "{\n",
    "    \"driverMemory\": \"10000M\",\n",
    "    \"driverCores\": 6,\n",
    "    \"executorMemory\": \"16000M\",\n",
    "    \"conf\" : {\"spark.executor.instances\":8, \"spark.executor.cores\": 6, \"spark.task.cpus\": 6, \"spark.yarn.maxAppAttempts\": 1, \"spark.yarn.executor.memoryOverhead\": \"16G\", \"spark.sql.files.maxPartitionBytes\": 4294967296, \"spark.dynamicAllocation.enabled\": false},\n",
    "    \"jars\" : [\"https://repo1.maven.org/maven2/ai/rapids/cudf/0.9.2/cudf-0.9.2.jar\",\n",
    "      \"https://repo1.maven.org/maven2/ai/rapids/xgboost4j-spark_2.x/1.0.0-Beta5/xgboost4j-spark_2.x-1.0.0-Beta5.jar\",\n",
    "      \"https://repo1.maven.org/maven2/ai/rapids/xgboost4j_2.x/1.0.0-Beta5/xgboost4j_2.x-1.0.0-Beta5.jar\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.listJars.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// import notebook source\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}\n",
    "import ml.dmlc.xgboost4j.scala.spark.{XGBoostClassifier, XGBoostClassificationModel}\n",
    "import ml.dmlc.xgboost4j.scala.spark.rapids.{GpuDataReader, GpuDataset}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//For AWS us-east-1 region, please use belowing S3 bucket\n",
    "//For AWS us-west-2 region, please use DATA_PREFIX = \"s3://spark-xgboost-mortgage-dataset-east1/csv/\"\n",
    "\n",
    "val DATA_PREFIX = \"s3://spark-xgboost-mortgage-dataset-east1/csv/\"\n",
    "val trainPath = DATA_PREFIX + \"train/20*\"\n",
    "val evalPath  = DATA_PREFIX + \"eval/20*\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.listJars.foreach(println)\n",
    "\n",
    "val spark = SparkSession.builder.appName(\"mortgage-gpu\").getOrCreate\n",
    "\n",
    "val dataReader = new GpuDataReader(spark)\n",
    "\n",
    "val labelColName = \"delinquency_12\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val schema = StructType(List(\n",
    "    StructField(\"orig_channel\", DoubleType),\n",
    "    StructField(\"first_home_buyer\", DoubleType),\n",
    "    StructField(\"loan_purpose\", DoubleType),\n",
    "    StructField(\"property_type\", DoubleType),\n",
    "    StructField(\"occupancy_status\", DoubleType),\n",
    "    StructField(\"property_state\", DoubleType),\n",
    "    StructField(\"product_type\", DoubleType),\n",
    "    StructField(\"relocation_mortgage_indicator\", DoubleType),\n",
    "    StructField(\"seller_name\", DoubleType),\n",
    "    StructField(\"mod_flag\", DoubleType),\n",
    "    StructField(\"orig_interest_rate\", DoubleType),\n",
    "    StructField(\"orig_upb\", IntegerType),\n",
    "    StructField(\"orig_loan_term\", IntegerType),\n",
    "    StructField(\"orig_ltv\", DoubleType),\n",
    "    StructField(\"orig_cltv\", DoubleType),\n",
    "    StructField(\"num_borrowers\", DoubleType),\n",
    "    StructField(\"dti\", DoubleType),\n",
    "    StructField(\"borrower_credit_score\", DoubleType),\n",
    "    StructField(\"num_units\", IntegerType),\n",
    "    StructField(\"zip\", IntegerType),\n",
    "    StructField(\"mortgage_insurance_percent\", DoubleType),\n",
    "    StructField(\"current_loan_delinquency_status\", IntegerType),\n",
    "    StructField(\"current_actual_upb\", DoubleType),\n",
    "    StructField(\"interest_rate\", DoubleType),\n",
    "    StructField(\"loan_age\", DoubleType),\n",
    "    StructField(\"msa\", DoubleType),\n",
    "    StructField(\"non_interest_bearing_upb\", DoubleType),\n",
    "    StructField(labelColName, IntegerType)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// you can set max_depth to 8 and num_round to 100 to shorten the training time\n",
    "\n",
    "val commParamMap = Map(\n",
    "    \"eta\" -> 0.1,\n",
    "    \"gamma\" -> 0.1,\n",
    "    \"missing\" -> 0.0,\n",
    "    \"max_depth\" -> 20,\n",
    "    \"max_leaves\" -> 256,\n",
    "    \"grow_policy\" -> \"depthwise\",\n",
    "    \"min_child_weight\" -> 30,\n",
    "    \"lambda\" -> 1,\n",
    "    \"scale_pos_weight\" -> 2,\n",
    "    \"subsample\" -> 1,\n",
    "    \"nthread\" -> 6,\n",
    "    \"num_round\" -> 1000,\n",
    "    \"num_workers\" -> 8,\n",
    "    \"tree_method\" -> \"gpu_hist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (trainSet, evalSet) = {\n",
    "  dataReader.option(\"header\", true).schema(schema)\n",
    "  (dataReader.csv(trainPath), dataReader.csv(evalPath))}\n",
    "\n",
    "val featureNames = schema.filter(_.name != labelColName).map(_.name)\n",
    "\n",
    "object Benchmark {\n",
    "  def time[R](phase: String)(block: => R): (R, Float) = {\n",
    "    val t0 = System.currentTimeMillis\n",
    "    val result = block // call-by-name\n",
    "    val t1 = System.currentTimeMillis\n",
    "    println(\"==> Benchmark: Elapsed time for [\" + phase + \"]: \" + ((t1 - t0).toFloat / 1000) + \"s\")\n",
    "    (result, (t1 - t0).toFloat / 1000)\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val modelPath = \"/tmp/model\"\n",
    "val xgbClassifier = new XGBoostClassifier(commParamMap).setLabelCol(labelColName).setFeaturesCols(featureNames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"\\n------ Training ------\")\n",
    "val (model, _) = Benchmark.time(\"train\") {\n",
    "        xgbClassifier.fit(trainSet)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Save model if modelPath exists\n",
    "model.write.overwrite().save(modelPath)\n",
    "val xgbClassificationModel = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"\\n------ Transforming ------\")\n",
    "val (results, _) = Benchmark.time(\"transform\") {\n",
    "  xgbClassificationModel.transform(evalSet)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"\\n------Accuracy of Evaluation------\")\n",
    "val evaluator = new MulticlassClassificationEvaluator().setLabelCol(labelColName)\n",
    "val accuracy = evaluator.evaluate(results)\n",
    "println(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
